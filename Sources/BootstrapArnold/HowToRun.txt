Cluster Setup
-------------

1) Ensure that your Condor server and worker machines are all installed and configured according to:
https://sites.google.com/a/keenswh.com/ai/projects/brain-simulator/brain-simulator-cloud-solution/batch-processing
2) Copy and run FirewallExceptions.cmd with admin privileges on all worker machines.
3) Ensure it is possible to use C:\arnold directory on all worker machines.

Scenario Preparation
--------------------

1) Choose a subset of worker machines for the scenario and edit Inputs\nodelist.txt accordingly.
2) Edit also 'requirements' section and 'queue' command within BootstrapArnold.condor to match machine names and machine count from Inputs\nodelist.txt.
3) Open and build ..\core\core\core.sln in release mode.
4) Open ..\UI\ArnoldUI.sln and ensure its binding to core is configured as follows:
- core binary directory shall be set to C:\arnold
- core command shall contain '++nodelist nodelist.txt' among its arguments
- core command +p and ++ppn arguments shall correspond to the characteristics of machines within nodelist.txt (e.g. to maximally utilize 2 machines each having 4 cores, arguments shall be '+p8 ++ppn 4' to run 8 Charm workers split into 2 processes each having 4 threads)
5) Ensure Results\checkpoint directory exists and is empty.

Scenario Execution
------------------

1) Run command line console and change the directory to the directory where this text file is located (i.e. BootstrapArnold).
2) Submit the Condor job to the cluster via the following command:
condor_submit -name HTCONDOR-CM -spool BootstrapArnold.condor
3) Take a note of what ID the job was given (let's assume 38 for this tutorial).
4) Using condor_status command, wait until all worker machines mentioned in Inputs\nodelist.txt becomes Claimed and Busy. This means that core binaries and checkpoint files were successfully copied there and charmd is running on each of them.
5) Run ArnoldUI and launch core via a corresponding button, see the console windows whether it properly initilized and if so, start the simulation and let it run for a while (at least until the first checkpoint is generated).
6) Pause the simulation, tear down the core via a corresponding button in UI, and close the ArnoldUI.
7) In order to terminate charmd on worker machines and gather all parts of the checkpoint to the Condor server, use the following command:
condor_vacate_job -name HTCONDOR-CM 38
8) Use the following helper script to download and merge the checkpoint parts from the Condor server (third argument is the number of worker machines mentioned in Inputs\nodelist.txt):
GatherCheckpoint.cmd HTCONDOR-CM 38 2
9) Note that the job was forcefully vacated, so to remove it from the queue of scheduled jobs and let future jobs run, it is necessary to use the following command:
condor_rm -name HTCONDOR-CM 38

Scenario Restarting
-------------------

1) Results\checkpoint shall now contain a checkpoint from the previous run.
2) Open ..\UI\ArnoldUI.sln and edit its binding to the core the following way:
- core command shall contain '+restart checkpoint' among its arguments
3) Now you can repeatedly go through steps 1) to 9) from the previous section, each time resuming the simulation from the last checkpoint.
